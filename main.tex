
\documentclass[11pt]{article}

\input{macro}

\usepackage{titling}

\usepackage{times}

\usepackage{setspace}
%\doublespacing

\setlength{\droptitle}{-5.5em} 

\setlength{\topmargin}{-.6in} 
\setlength{\textheight}{9in}
\setlength{\textwidth}{7in} 
\setlength{\headheight}{26pt}
\setlength{\headsep}{9pt} 
\setlength{\oddsidemargin}{-.26in}
\setlength{\evensidemargin}{.25in}

\renewcommand{\footnotesize}{\scriptsize}

\begin{document}

\title{Research Statement}
\author{Tanakorn Leesatapornwongsa}
\date{\vspace{-1ex} \small{Department of Computer Science, University of
Chicago}}

\maketitle

My research focuses on improving the dependability of ``\textit{cloud-scale}''
distributed systems such as scale-out storage systems, distributed computing
frameworks, and cluster management services. Users demand 24/7 dependability of
cloud computing systems. Unfulfilled dependability is costly. Internet service
companies collectively lose billions of dollars in revenue each year from
service downtimes. Yet, there are complex challenges to reach an ideal
dependability. Behind cloud computing is a collection of hundreds of complex
systems written in millions of lines of code that are brittle and prone to
failures.

My main focus is on one unsolved reliability problem in cloud systems that is
\textbf{\textit{distributed concurrency (DC) bugs}}. DC bugs are caused by
nondeterministic order of distributed events such as message arrivals, hardware
crashes and reboots, and other arbitrary faults. Cloud systems execute multiple
complicated distributed protocols concurrently (\eg, serving users' requests,
operating some background tasks, and combined with untimely hardware failures).
The possible interleavings of the distributed events are beyond developer's
expectations and some interleavings might not be handled properly.  The buggy
distributed interleavings can cause catastrophic failures such as data loss,
data inconsistencies and downtimes.

In the following sections, I explain how I have addressed DC bugs, started by a
formal bug study, and building distributed systems model checker. And I also
describe my other works on other aspects of dependability research.

\section{Distributed Concurrency (DC) Bugs}\label{dcbugs}

Compared to the countless efforts in combating \textbf{\textit{local concurrency
(LC) bugs}}, which occur due to concurrency of multithreading,
DC bugs have not received the same amount of attention. This is time for the
dependability community to address this important problem in a systematic and
comprehensive manner. To combat DC bugs, I establish my research in (1)
\textbf{\textit{formal bug studies}} and (2) \textbf{\textit{distributed system
model checking}}. 

\subsection{DC Bug Study \& Taxonomy} 

Bug or failure studies can significantly guide many aspects of dependability
research, and introduce new research problem. Many dependability researchers
have recently employed formal studies on bugs and failures such as the studies
on large-scale system bugs/failures from Microsoft \footnote{Zhenyu Guo, Sean
McDirmid, Mao Yang, Li Zhuang, Pu Zhang, Yingwei Luo, Tom Bergan, Madan
Musuvathi, Zheng Zhang, and Lidong Zhou.  Failure Recovery: When the Cure Is
Worse Than the Disease. HotOS '13} \footnote{Sihan Li, Hucheng Zhou, Haoxiang
Lin, Tian Xiao, Haibo Lin, Wei Lin, and Tao Xie. A Characteristic Study On
Failures of Production Distributed Data-Parallel Programs. ICSE '13}.  Thus, I
started my research by doing formal bug study to gain foundations of combating
DC bugs.

\if 0
\subsubsection{Cloud Bug Study}

As an initiative, our group have performed the largest bugs study in six
important Apache cloud infrastructures including Cassandra, Flume, Hadoop
MapReduce, HBase, HDFS, and ZooKeeper \cite{Gunawi+14-Cbs}. We reviewed in
total 21,399 submitted issues within a three-year period (2011-2014) in Apache
bug repositories. We perform a deep analysis of 3,655 ``vital'' issues (\ie,
real issues affecting deployments) with a set of detailed classifications. 
\fi

\if 0
This
work led us to several interesting dependability research questions, and was
the main source of my DC-bug taxonomy work.
\fi

While there have been many LC-bug studies, I am not aware of any large-scale
study of DC bugs. A recent study from Microsoft analyzed the effect of
distributed concurrency on workload and only studied five DC bugs in MapReduce
systems \footnote{Tian Xiao, Jiaxing Zhang, Hucheng Zhou, Zhenyu Guo, Sean
McDirmid, Wei Lin, Wenguang Chen, and Lidong Zhou. Nondeterminism in MapReduce
Considered Harmful?  An Empirical Study on Non-commutative Aggregators in
MapReduce Programs. ICSE '14}. To fill the void, I as one of the project
leaders, have created the largest and most comprehensive taxonomy of 104
real-world DC bugs (named \taxdc) from Cassandra, HBase, Hadoop MapReduce/Yarn,
and ZooKeeper \cite{Gunawi+16-TaxDc-Appear}. \taxdc\ contains in-depth
characteristics of DC bugs, stored in the form of 2,083 classification labels
and 4,528 lines of re-enumerated steps to the bugs that I manually added.
Motivated by the availability of bug benchmarks for LC bugs, I release
\taxdc\ as a large-scale DC bugs benchmark.

With \taxdc\, I can answer important questions such as: How often are DC bugs 
reported from real deployments? What types of DC bugs exist in real world?
What are the root causes of DC bugs (out-of-order messages, failures, \etc)?
Are existing LC-bug-detection tools applicable for DC bugs? How do developers
fix DC bugs (by adding locks, states, \etc)? What are the inputs/triggering
conditions?  What are the minimum number of distributed events needed to
trigger the bugs (how many messages to re-order, failures to inject, \etc)?
What errors/effects (specification violations) are caused by DC bugs (deadlock,
data loss, state inconsistency, performance problems, \etc)? How do propagation
chains form from the root causes to errors? The answers to these questions will
guide my subsequent research projects.

\subsection{Distributed System Model Checking}

One powerful method for discovering hidden DC bugs is the use of an
\textit{implementation-level distributed system model checker} (\textbf{dmck}).
By re-ordering nondeterministic distributed events, a dmck can discover buggy
interleavings that lead to DC bugs. The last eight years have seen a rise of
dmcks such as MaceMC \footnote{Charles Killian, James Anderson, Ranjit Jhala,
and Amin Vahdat. Life, Death, and the Critical Transition: Finding Liveness Bugs
in Systems Code. NSDI '07}, \modist\ \footnote{Junfeng Yang, Tisheng Chen, Ming
Wu, Zhilei Xu, Xuezheng Liu, Haoxiang Lin, Mao Yang, Fan Long, Lintao Zhang, and
Lidong Zhou. MODIST: Transparent Model Checking of Unmodified Distributed
Systems. NSDI '09}, or Demeter \footnote{Huayang Guo, Ming Wu, Lidong Zhou, Gang
Hu, Junfeng Yang, and Lintao Zhang. Practical Software Model Checking via
Dynamic Interface Reduction. SOSP '11}. One big challenge faced by a dmck is the
state-space explosion problem (\ie, there are too many distributed events to
re-order). To address this, existing dmcks adopt a random walk or basic
reduction techniques such as dynamic partial order reduction (DPOR). Despite
these early successes, existing approaches cannot unearth many real-world DC
bugs, so I am advancing the state of the art of dmck to combat DC bugs, which I
describe below.

\subsubsection*{Semantic-Aware Model Checking (Initial Work)} 

My work started by specifically addressing two limitations of existing dmcks.
First, existing dmcks treat every target system as a complete \textit{black
box}, and perform unnecessary reorderings of distributed events that
would lead to the same states (\ie, redundant executions). Second,
they do not incorporate complex multiple fault events (\eg, crashes, reboots)
into their exploration strategies, as such inclusion would exacerbate the
state-space explosion problem.

To address these limitations, I built Semantic-Aware Model Checking
(\textbf{SAMC}) \cite{Leesatapornwongsa+15-SamcIssta,Leesatapornwongsa+14-Samc},
a novel white-box model checking approach that takes \textit{semantic knowledge}
of how distributed events (specifically, messages, crashes, and reboots) are
processed by the target system and incorporates that to create reduction
policies. The policies are based on sound reduction techniques such as DPOR and
symmetry. The policies tell SAMC not to re-order some pairs of events such as
message-message pairs, and message-crash pairs, yet preserves soundness, because
those cut out re-orderings are redundant. 

I built SAMC from scratch, I was able to reproduce 12 old bugs
in 3 cloud systems involving 30-120 distributed events and multiple crashes and
reboots. Some of these bugs cannot be unearthed by non-SAMC approaches, even
after two days. SAMC can find the bugs up to 271x (33x on average) faster
compared to state-of-the-art techniques. Additionally, I found two new bugs in
Hadoop MapReduce and ZooKeeper. Although, SAMC was built for checking
distributed systems, \textit{the principle of \textbf{semantic awareness} I
introduced is also applicable to multithreading software model cheker as well}.

\subsubsection*{Full Semantic-Aware Model Checking (Ongoing Work)} 

There are two major gaps between existing dmcks (including SAMC) and real-world
DC bugs. First, dmcks reorder messages by default, but they do not control the
timings of all types of events necessary for DC bugs. For example, MaceMC
and SAMC do not intercept local computation and do not exercise timeouts;
\modist\ and Demeter do not inject multiple crash and reboot timings; and none
of the above include other faults such as untimely disk faults.
%
Second, controlling all necessary events will ``blow up'' the exploration space.
Thus, more innovations are needed to devise fast exploration strategies.

%that leverage semantic relationships among all necessary events.

Demeter, the latest state of the art for exercising message-computation race,
still hits a scalability wall and the authors hint that using semantic knowledge
is an important future direction. Thus, to address the problem, I am building
\fullcheck, a dmck that intercepts all necessary events to unearth DC bugs, but
will do so in a fast and scalable manner. I will invent more powerful
semantic-awareness principles while adopting new reduction techniques in the
building of \fullcheck.

%\fullcheck\ will adopt more advanced reduction techniques assisted by
%the incorporation of semantic relationships between the events. 

\if 0
To address the problem, I am building \fullcheck, a dmck that intercepts all
necessary events to unearth DC bugs, but will do so in a fast and scalable
manner. \fullcheck\ will adopt more advanced reduction techniques assisted by
the incorporation of semantic relationships between the events. 

More reduction techniques are needed, but the semantic-awareness is still the
most important. Demeter, the latest state of the art for exercising
message-computation race, still hits a scalability wall and the authors hint
that using semantic knowledge is an important future direction. I will build
more powerful semantic-awareness principles while adopting new reduction
techniques in the building of \fullcheck.
\fi

\if 0
For example, bounded model checking is a popular technique,
which explores only limited depth of distributed events to avoid state-space
explosion, could be useful for dmck, but integration must be done in a wise
manner, because it works well for bugs hiding in early steps of execution only.
Previous works from Microsoft showed that bounded model checking can work with
LC-bug model checking effectively by introducing \textit{iterative context
bounding} \footnote{Madan Musuvathi, and Shaz Qadeer. Iterative Context Bounding
for Systematic Testing of Multithreaded Programs. PLDI '07} and \textit{bounded
partial-order reduction} \footnote{Katherine E. Coons, Madanlal Musuvathi, and
Kathryn S. McKinley. Bounded Partial-Order Reduction. OOPSLA '13}. For dmck, to
effectively integrate bounded model checking, we need a subtle strategy to make
sure that we still explore deep enough to reach hidden bugs.
\fi

% I could talk about other reduction technique here, maybe Demeter, bounded
% model checking from MSR, etc.

\subsubsection*{Automated Semantic-Aware Model Checking (Ongoing Work)} 

So far, as we leverage domain-specific semantic information into reduction
strategies, we (or the developers) must manually extract and incorporate the
semantic knowledge and write the corresponding reduction policies. This manual
process, based on high-level human understanding of the codebase, can
potentially miss important patterns due to human errors and breaks soundness.

To address the unsoundness of SAMC and the developers' burden in manually
writing semantic-based reduction strategies, I am creating \autocheck, a dmck
that automatically and soundly extracts complete semantic knowledge into
reduction strategies with the help of program analysis. To do so, I combine
symbolic execution and dmck. While others have used symbolic execution with
model checking for LC bugs, \autocheck\ will be the first case for
implementation-level dmck. 

\subsubsection*{Deep Semantic-Aware Model Checking (Future Work)}

Execution paths to DC bugs often require complex input preconditions such as
multiple faults, reboots, and protocol initiations. I found that more than 60\%
of DC bugs require more than one protocol initiation, 35\% require multiple
faults, and 29\% of DC bugs arise within buggy interactions between foreground
and background protocols.  This again highlights the complexity of complete
systems. If we do not include the complex preconditions, the bugs will not
surface in checking process.

To address this complexity, I will construct \deepcheck, a dmck with a backward
static analysis tool that is capable of searching the necessary input
preconditions to cover unreachable paths. The concept of \deepcheck\ is the dmck
will run with a limited input precondition. Then, \deepcheck\ will analyze which
code path is not reachable given the limited input. It will perform a backward
analysis to search for input preconditions to the path. As a result, this
backward analysis will provide the sequence of input preconditions that cover
more complex scenarios.

\section{Other Works}

Other than DC bugs, I joined a project performing the largest bugs study in six
important Apache cloud infrastructures including Cassandra, Flume, Hadoop
MapReduce, HBase, HDFS, and ZooKeeper \cite{Gunawi+14-Cbs}. We reviewed in total
21,399 submitted issues within 2011-2014 in Apache bug repositories. We perform
a deep analysis of 3,655 ``vital'' issues (\ie, real issues affecting
deployments) with a set of detailed classifications. This study show new types
of bugs that are \textit{specific} to cloud-scale distributed systems only. 

\subsection{Scalability Bugs}

From cloud-specific bugs my study revealed, I have address one disregarded type
of bug to address, which is scalability bugs. They are latent bugs that are
scale-dependent; they only surface in large-scale deployments, but not in
small/medium-scale ones. Their presence jeopardizes systems reliability and
availability at scale. 

\if 0
From this, I can identify a new types of bugs that are \textit{specific} to
scalability aspect of cloud-scale distributed systems, but not much attention
paid on them, which I call ``\textbf{scalability bugs}''. They are latent bugs
that are scale-dependent; they only surface in large-scale deployments, but not
in small/medium-scale ones. Their presence jeopardizes systems reliability and
availability at scale. 
\fi 

To address this problem, I started a pilot work on scalability checking. I
introduced \sck, a methodology that enables developers to scale-check
distributed systems and find scalability bugs on \textit{one machine}. With
\sck, I can reproduce 6 scalability bugs in Cassandra, Riak, and Voldemort.
These bugs manifest in large scale only (\eg 200-500 machines).  The work is in
submission to HotOS '17.

\section{Summary}

I envision the future of cloud-scale distributed systems to be ``non-stop''. I
am improving dependability of systems by addressing well-known unsolved problem
of DC bugs via model checker, and tackling a new class of scalability bugs that
the systems community rarely pay attention to. A journey toward the goal is a
long road, but I believe my research advances systems community toward this
direction. 

\input{bib}

\end{document}
