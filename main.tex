
\documentclass[11pt]{article}

\input{macro}

\usepackage{titling}

\usepackage{times}

\usepackage{setspace}
%\doublespacing

\setlength{\droptitle}{-5.5em} 

\setlength{\topmargin}{-.6in} 
\setlength{\textheight}{9in}
\setlength{\textwidth}{7in} 
\setlength{\headheight}{26pt}
\setlength{\headsep}{9pt} 
\setlength{\oddsidemargin}{-.26in}
\setlength{\evensidemargin}{.25in}

\renewcommand{\footnotesize}{\scriptsize}

\begin{document}

\title{Research Statement}
\author{Tanakorn Leesatapornwongsa}
\date{\vspace{-1ex} \small{Department of Computer Science, University of
Chicago}}

\maketitle

My research focuses on improving the dependability of ``\textit{cloud-scale}''
distributed systems such as scale-out storage systems, and distributed computing
frameworks. Users demand 24/7 dependability of
cloud services. Unfulfilled dependability is costly. Internet service companies
collectively lose billions of dollars in revenue each year from service
downtimes. Yet, there are complex challenges to reach an ideal dependability. 

My main focus is on one unsolved reliability problem in cloud systems that is
\textbf{\textit{distributed concurrency (DC) bugs}}. DC bugs are caused by
nondeterministic orders of distributed events such as message arrivals, and
hardware crashes and reboots. Cloud systems execute multiple complicated
distributed protocols concurrently (\eg, serving requests, operating
background tasks, and combined with untimely hardware failures). The possible
interleavings of the distributed events are beyond developers' expectations and
some interleavings might not be handled properly. The buggy distributed
interleavings can cause catastrophic failures such as data loss, and downtimes.

In the section 1 below, I explain how I have addressed DC bugs, by (1)
\textit{\textbf{a formal bug study}}, and (2) \textit{\textbf{advancing
distributed system model checker}}. I also describe my works on other aspects
of cloud dependability in section 2.

%In the following sections, I explain how I have addressed DC bugs, by (1)
%\textit{\textbf{a formal bug study}}, and (2) \textit{\textbf{distributed
%systems model checker}}. And I also describe my other works on other aspects of
%cloud dependability.

\section{Distributed Concurrency (DC) Bugs}\label{dcbugs}

%To combat DC bugs, I establish my research in (1) \textbf{\textit{formal bug
%studies}} and (2) \textbf{\textit{distributed system model checking}}. 

\subsection{DC Bug Study \& Taxonomy} 

Bug studies can significantly guide many aspects of dependability
research, and introduce new research problems. Many dependability researchers
have recently employed formal studies on bugs and failures such as the studies
on large-scale system bugs/failures from Microsoft \footnote{Zhenyu Guo, Sean
McDirmid, Mao Yang, Li Zhuang, Pu Zhang, Yingwei Luo, Tom Bergan, Madan
Musuvathi, Zheng Zhang, and Lidong Zhou.  Failure Recovery: When the Cure Is
Worse Than the Disease. HotOS '13} \footnote{Sihan Li, Hucheng Zhou, Haoxiang
Lin, Tian Xiao, Haibo Lin, Wei Lin, and Tao Xie. A Characteristic Study On
Failures of Production Distributed Data-Parallel Programs. ICSE '13}.

However, I am not aware of any large-scale DC-bug study. A recent study from
Microsoft analyzed the effect of distributed concurrency of workload and only
studied five DC bugs in MapReduce \footnote{Tian Xiao, Jiaxing Zhang, Hucheng
Zhou, Zhenyu Guo, Sean McDirmid, Wei Lin, Wenguang Chen, and Lidong Zhou.
Nondeterminism in MapReduce Considered Harmful?  An Empirical Study on
Non-commutative Aggregators in MapReduce Programs. ICSE '14}. To fill the void,
I as one of the project leaders, created the largest and most comprehensive
taxonomy of 104 real-world DC bugs (named \taxdc) from four popular Apache
projects \cite{Leesatapornwongsa+16-TaxDC}. \taxdc\ contains in-depth
characteristics of DC bugs, and re-enumerating steps that I manually added.  It
was publicly released as the first large-scale DC bugs benchmark.

%Motivated by the availability of bug benchmarks for LC bugs, I release \taxdc\
%as a large-scale DC bugs benchmark.

With \taxdc\, I can answer important questions such as: 
What are the root causes of DC bugs?
%What are the inputs/triggering conditions? 
%What errors/effects are caused by DC bugs?
How do developers fix DC bugs? 
Are existing concurrency-bug-detection tools applicable for DC bugs? 
The answers to these questions will guide my subsequent research projects.

\subsection{Distributed System Model Checking}

One powerful method for discovering hidden DC bugs is the use of an
\textit{implementation-level distributed system model checker} (\textbf{dmck}).
By re-ordering nondeterministic distributed events, a dmck can discover buggy
interleavings that lead to DC bugs. The last eight years have seen a rise of
dmcks such as 
%MaceMC \footnote{Charles Killian, James Anderson, Ranjit Jhala,
%and Amin Vahdat. Life, Death, and the Critical Transition: Finding Liveness Bugs
%in Systems Code. NSDI '07}, 
\modist\ \footnote{Junfeng Yang, Tisheng Chen, Ming
Wu, Zhilei Xu, Xuezheng Liu, Haoxiang Lin, Mao Yang, Fan Long, Lintao Zhang, and
Lidong Zhou. MODIST: Transparent Model Checking of Unmodified Distributed
Systems. NSDI '09}, or Demeter \footnote{Huayang Guo, Ming Wu, Lidong Zhou, Gang
Hu, Junfeng Yang, and Lintao Zhang. Practical Software Model Checking via
Dynamic Interface Reduction. SOSP '11}. One big challenge faced by a dmck is the
state-space explosion problem (\ie, there are too many distributed events to
re-order). To address this, existing dmcks adopt a random walk or basic
reduction techniques such as dynamic partial order reduction (DPOR). Despite
these early successes, existing approaches cannot unearth many real-world DC
bugs, so I am advancing the state of the art of dmck to combat DC bugs, which I
describe below.

\subsubsection*{Semantic-Aware Model Checking (Initial Work)} 

My work started by specifically addressing two limitations of existing dmcks.
First, existing dmcks treat every target system as a complete \textit{black
box}, and perform unnecessary reorderings of distributed events that
would lead to the same states (\ie, redundant executions). Second,
they do not incorporate complex multiple fault events (\eg, crashes, reboots)
into their exploration strategies, as such inclusion would exacerbate the
state-space explosion problem.

To address these limitations, I built Semantic-Aware Model Checking
(\textbf{SAMC}) \cite{Leesatapornwongsa+15-SamcIssta,Leesatapornwongsa+14-Samc},
a novel white-box model checking approach that takes \textit{semantic knowledge}
of how distributed events (specifically, messages, crashes, and reboots) are
processed by the target system and incorporates that to create reduction
policies. The policies are based on sound reduction techniques such as DPOR and
symmetry. The policies tell SAMC not to re-order some pairs of events such as
message-message pairs, and message-crash pairs, yet preserves soundness, because
those cut out re-orderings are redundant. 

I built SAMC from scratch, I was able to reproduce 12 old bugs
in 3 cloud systems involving 30-120 distributed events and multiple crashes and
reboots. Some of these bugs cannot be unearthed by non-SAMC approaches, even
after two days. SAMC can find the bugs up to 271x (33x on average) faster
compared to state-of-the-art techniques. Additionally, I found two new bugs in
Hadoop MapReduce and ZooKeeper. Although, SAMC was built for checking
distributed systems, the principles of \textbf{semantic awareness} I
introduced is also applicable to multithreading software model cheker as well.

\subsubsection*{Full Semantic-Aware Model Checking (Ongoing Work)} 

There are two major gaps between existing dmcks and real-world
DC bugs. First, dmcks reorder messages by default, but they do not control the
timings of all events necessary for DC bugs. For example, 
SAMC do not intercept local computation and do not exercise timeouts;
\modist\ and Demeter do not inject multiple crash and reboot timings; and none
of the above include other faults such as untimely disk faults.
%
Second, controlling all necessary events will ``blow up'' the exploration space.
Thus, more innovations are needed to devise fast exploration strategies.

%that leverage semantic relationships among all necessary events.

Demeter, the latest state of the art for exercising message-computation race,
still hits a scalability wall and the authors hint that using semantic knowledge
is an important future direction. To address the problem, I am building
\fullcheck, a dmck that intercepts all necessary events to unearth DC bugs, but
will do so in a fast and scalable manner. I am inventing more powerful
semantic-awareness principles while adopting new reduction techniques in the
building of \fullcheck.

%\fullcheck\ will adopt more advanced reduction techniques assisted by
%the incorporation of semantic relationships between the events. 

% I could talk about other reduction technique here, maybe Demeter, bounded
% model checking from MSR, etc.

\subsection{Future Work}

\fullcheck\ will make bug checking practical, but it still requires manual
efforts from developers. My future work will automate the manual processes
to reduce the burdensome, as I explain next.

\subsubsection*{Automated Semantic Extractor}

So far, developers manually extract and incorporate the semantic knowledge to
SAMC. This process could introduce human errors and breaks soundness. Thus, I
plan to advance SAMC to automatically extracts complete semantic
knowledge into reduction strategies with the help of program analysis. To do
so, I adopt symbolic execution to create the semantic-aware reduction policies.
While others have used symbolic execution with model checking for LC bugs, this
work will be the first case for implementation-level dmck. 

\if 0
So far, leveraging domain-specific semantic information to dmck requires
developers to manually extract and incorporate the semantic knowledge. This
manual process can potentially introduce human errors and breaks soundness.
Thus, I plan to advance SAMC to automatically and soundly extracts complete
semantic knowledge into reduction strategies with the help of program analysis.
To do so, I adopt symbolic execution to create the semantic-aware reduction
policies. While others have used symbolic execution with model checking for LC
bugs, this work will be the first case for implementation-level dmck. 
\fi

\subsubsection*{Automatic Workload Generator}

TaxDC shows 60\% of DC bugs require more than one request workload,
35\% require multiple faults, and 29\% arise because
interactions between foreground and background operations. This again highlights
the complexity of complete systems. If we do not include the complex
preconditions, the bugs will not surface in checking process. To address this
complexity, I will adopt static and dynamic analysis to generate the necessary
input preconditions to cover more complex test scenarios.

\section{Other Works}

Other than DC bugs, I joined a project performing the largest bugs study to find
``\textit{what bugs live in the cloud?}'' \cite{Gunawi+14-Cbs}. We studied six
popular Apache cloud infrastructures (\eg\ Cassandra, Hadoop MapReduce, and
HBase). We reviewed in total 21,399 submitted issues within 2011-2014, and
perform a deep analysis of 3,655 ``\textit{vital}'' issues. This study show novel
dimensions of bugs that are \textit{specific} to cloud systems only. Motivated
by the discovery, I have been working in many projects to tackle the
bugs in the cloud, which I explain below.

\subsection{Scalability Bugs}

Scalability bugs are latent bugs that are scale-dependent; they only surface in
large-scale deployments, but not in small/medium-scale ones. Their presence
jeopardizes systems reliability and availability at scale. 
%
To address this problem, I started a pilot work on scalability checking
\cite{Gunawi+17-ScaleCheck-Insub}. I introduced \sck, a methodology that enables
developers to check and debug scalability bugs on \textit{one machine}. With
\sck, I can reproduce 6 scalability bugs in Cassandra, Riak, and Voldemort.
These bugs manifest in large scale only (\eg\ 200-500 machines). 

\subsection{Limpware Tolerant Computing}

Hardware can fail in different ways such as corruption, fail stop, and fail
partial, but there is one often-overlooked cause of performance instability:
``\textbf{\textit{limpware}}'', \textit{limping} hardware whose performance
degrades significantly compared to its specification.  There was no works
studied the impact of this overlooked limpware failure, so my colleagues and I
benchmarked eight popular Apache cloud systems with limpware injections
\cite{Do+13-Limplock}. The result exposes the inability of today's cloud
systems in dealing with limpware. A single limpware can cripple other healthy
nodes, or even worse, a whole cluster.  Motivated from this observation, we
introduced a robust Path-Based Speculative Execution (\textbf{PBSE}) for
data-parallel frameworks, that can recover from limping NICs
\cite{Suminto+17-PBSE-InSub}.

\subsection{Drill-Ready Cloud} 

Faults in real deployments are very complex. The real environments could be
unforeseen (multi datacenters, layers of virtualization, massive
workloads, \etc), but in offline testings, the environments are much simpler, and
the scale of the workloads and injected faults are often orders of magnitude
smaller. To reach ideal dependability, I envision the future of cloud testing to
be \textit{online}, and introduced ``\textbf{failure drill}'', the concepts of
how online testing for cloud should be operated
\cite{Leesatapornwongsa+14-Drill-fixed}.  The failure drill addresses how online
test can be safe, efficient, usable, and general.

\section{Summary}

I envision the future of cloud-scale distributed systems to be ``non-stop''. I
am improving dependability of systems by addressing well-known unsolved problem
of DC bugs via model checker, and tackling new dimensions of cloud dependability 
that the systems community has not paid attention. A journey
toward the goal is a long road, but I believe my research advances systems
community toward the direction. 

\input{bib}

\end{document}
